{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafb354a-af6c-4276-8868-7ace94608a82",
   "metadata": {},
   "source": [
    "# begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24526d3e-b133-411c-99b1-ea52ee09b8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d6a27c-09cf-4232-af2c-91010bc85988",
   "metadata": {},
   "outputs": [],
   "source": [
    "randdata=[42, 168, 573, 81, 1094 , 9,\n",
    " 23, 1387, 937 ,1]  #  np.random.random_sample(10)\n",
    "#print(randdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56919cf6-e79a-4ff0-8e34-e0764148407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setting random seed\n",
    "SEED=randdata[0] \n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "def set_global_determinism(seed=SEED):\n",
    "    set_seeds(seed=seed)\n",
    "\n",
    "#     os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "#     os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "#     tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "#     tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Call the above function with seed value\n",
    "set_global_determinism(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602bd030-77cf-41e1-869d-d1290caa8fe9",
   "metadata": {
    "id": "H0ZgDQarWHcz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dicdata='../../Datasets/'\n",
    "dicnpy='../../dataUNSW/'\n",
    "dicoutcome='../../unsw_checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff592796-6c7a-418c-9bc4-366c6ca72cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def column_split_bina(ds):\n",
    "    '''split dataset to data_x,data_y for binary classification '''\n",
    "    if type(ds)==pd.DataFrame:\n",
    "        data_x=np.asarray(ds.iloc[:,:-2]).astype(np.float32)\n",
    "        data_y=np.asarray(ds.iloc[:,-1]).astype(np.float32)\n",
    "    elif type(ds)==np.ndarray:\n",
    "        data_x=ds[:,:-2].astype(np.float32)\n",
    "        data_y=ds[:,-1].astype(np.float32)        \n",
    "    else:\n",
    "        data_x=False\n",
    "        data_y=False\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e204702-9d5c-4525-a3be-7ac54e962b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training data\n",
    "try:\n",
    "    normal_train=np.load(dicnpy+'Normal_train_80_2.npy',allow_pickle=True)\n",
    "    normal_test=np.load(dicnpy+'Normal_test_20_2.npy',allow_pickle=True)\n",
    "except IOError:\n",
    "    print(\"Normal.npy didn't exist!\")\n",
    "    \n",
    "normal_train_x, _ =column_split_bina(normal_train)\n",
    "original_dim = normal_train_x.shape[1]\n",
    "input_shape = (original_dim,)\n",
    "\n",
    "klw=normal_train_x.shape[0]\n",
    "#intermediate_dim = int(original_dim / 2)\n",
    "latent_dim = int(original_dim / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd79f22-99a8-41e6-a84b-e8cc87e13b62",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7551f634-695c-4102-86af-e703df68a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_n_prior(shape, dtype=np.float32,independent=False):\n",
    "    if independent:\n",
    "        return   tfd.Independent(tfd.Normal(loc=tf.zeros(shape), scale=1),  reinterpreted_batch_ndims=1)\n",
    "    else:\n",
    "        return tfd.Normal(loc=tf.zeros(shape), scale=1)   # scale=0.01 ?\n",
    "    \n",
    "tfb = tfp.bijectors\n",
    "\n",
    "def make_mvn_prior(ndim, trainable=False, offdiag=False):\n",
    "        if not trainable:\n",
    "            if offdiag:\n",
    "                # With covariances\n",
    "                # Note: Diag must be > 0, upper triangular must be 0, and lower triangular may be != 0.\n",
    "                prior = tfd.MultivariateNormalTriL(\n",
    "                    loc=tf.zeros(ndim),\n",
    "                    scale_tril=tf.eye(ndim)\n",
    "                )\n",
    "            else:            \n",
    "                if True:  # kl_exact needs same dist types for prior and latent.\n",
    "                    prior = tfd.MultivariateNormalDiag(loc=tf.zeros(ndim), scale_diag=tf.ones(ndim))\n",
    "                else:\n",
    "                    prior = tfd.Independent(tfd.Normal(loc=tf.zeros(ndim), scale=1),\n",
    "                                            reinterpreted_batch_ndims=1)\n",
    "        else:\n",
    "            # Note, in TransformedVariable, the initial value should be that AFTER transform; weight trainable.\n",
    "            if offdiag:\n",
    "                prior = tfd.MultivariateNormalTriL(\n",
    "                    loc=tf.Variable(tf.random.normal([ndim], stddev=0.1, dtype=tf.float32),\n",
    "                                    name=\"prior_loc\"),\n",
    "                    scale_tril=tfp.util.TransformedVariable(\n",
    "                        tf.random.normal([ndim, ndim], mean=1.0, stddev=0.1, dtype=tf.float32),\n",
    "                        tfb.FillScaleTriL(), name=\"prior_scale\")\n",
    "                )\n",
    "            else:\n",
    "                scale_shift = np.log(np.exp(1) - 1).astype(np.float32)\n",
    "                prior = tfd.MultivariateNormalDiag(\n",
    "                    loc=tf.Variable(tf.random.normal([ndim], stddev=0.1, dtype=tf.float32),\n",
    "                                    name=\"prior_loc\"),\n",
    "                    scale_diag=tfp.util.TransformedVariable(\n",
    "                        tf.random.normal([ndim], mean=1.0, stddev=0.1, dtype=tf.float32),\n",
    "                        bijector=tfb.Chain([tfb.Shift(1e-5), tfb.Softplus(), tfb.Shift(scale_shift)]),\n",
    "                        name=\"prior_scale\"\n",
    "                    )\n",
    "                )                \n",
    "        return prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d1753-1466-49fb-99e0-0bafd18bedc2",
   "metadata": {},
   "source": [
    "##  dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0e7ec41-b3b6-4259-bddb-c643075fc8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               6144      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64)               256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,736\n",
      "Trainable params: 16,608\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 23:39:26.063901: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-13 23:39:28.419572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30984 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:61:00.0, compute capability: 7.0\n",
      "2023-09-13 23:39:28.421285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30984 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:62:00.0, compute capability: 7.0\n",
      "2023-09-13 23:39:28.422918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30984 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n",
      "2023-09-13 23:39:28.424544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30984 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0\n",
      "2023-09-13 23:39:29.162858: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.keras.initializers import RandomNormal, Constant\n",
    "#2layers (128,64,32) \n",
    "prior =tfd.Independent(tfd.Normal(loc=tf.zeros(32), scale=1)    , reinterpreted_batch_ndims=1)\n",
    "\n",
    "encoder=tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=input_shape),\n",
    "    tfkl.Dense(128,                \n",
    "        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),  \n",
    "        bias_initializer=  tf.zeros_initializer() ,\n",
    "               kernel_regularizer= tfpl.KLDivergenceRegularizer(make_mvn_prior(128),use_exact_kl=True, weight=1.0) ,   \n",
    "                bias_regularizer= tfpl.KLDivergenceRegularizer(make_mvn_prior(128),use_exact_kl=True, weight=1.0) ,  \n",
    "                           activation='relu'), \n",
    "\n",
    "    tfkl.Dense(64,\n",
    "        kernel_initializer=  tf.random_normal_initializer(mean=0, stddev= 0.01),    \n",
    "        bias_initializer=tf.zeros_initializer() ,\n",
    "                           kernel_regularizer= tfpl.KLDivergenceRegularizer(make_mvn_prior(64),use_exact_kl=True, weight=1.0) ,   \n",
    "                bias_regularizer= tfpl.KLDivergenceRegularizer(make_mvn_prior(64),use_exact_kl=True, weight=1.0) , \n",
    "                           activation='relu'), \n",
    "    tfkl.BatchNormalization(momentum=0.95, epsilon=0.005,\n",
    "                                beta_initializer=tf.random_normal_initializer(mean=0, stddev=0.05),   \n",
    "                                ),\n",
    "     \n",
    "    #  BAE\n",
    "    tfkl.Dense(32,\n",
    "        kernel_initializer=  tf.random_normal_initializer(mean=0, stddev=0.01),   \n",
    "        bias_initializer=tf.zeros_initializer() ,\n",
    "                kernel_regularizer=  tfpl.KLDivergenceRegularizer(make_mvn_prior(32),use_exact_kl=True, weight=1.0) ,   \n",
    "                bias_regularizer= tfpl.KLDivergenceRegularizer(make_mvn_prior(32),use_exact_kl=True, weight=1.0) ,  \n",
    "               activation='relu'),\n",
    "    \n",
    "#   ##  Here uncomment for BAE-Hetero-Latent\n",
    "#  tfp.layers.DistributionLambda(\n",
    "#     lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "#                    loc=t, scale_diag=1e-5 + tf.nn.softplus(np.log(np.expm1(1.)) + t) )) \n",
    "    \n",
    "#    ## Here uncomment for BAE-Hetero-Latent-VLB\n",
    "#  tfp.layers.DistributionLambda(\n",
    "#     lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "#                    loc=t, scale_diag=1e-5 + tf.nn.softplus(np.log(np.expm1(1.)) + t) ),\n",
    "#       activity_regularizer=tfpl.KLDivergenceRegularizer(make_mvn_prior(32),use_exact_kl=False, weight=1.0)) \n",
    "    \n",
    "])     \n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71a1445c-4776-46ba-adfd-b7769bbafc69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 32)]              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 47)                6063      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,495\n",
      "Trainable params: 16,495\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  decoder model \n",
    "latent_inputs =tfk. Input(shape=(32,), name='z_sampling')\n",
    "x = tfkl.Dense(64,\n",
    "        kernel_initializer= tf.random_normal_initializer(mean=0, stddev=0.01),   \n",
    "        bias_initializer=tf.zeros_initializer() ,   \n",
    "             kernel_regularizer=  tfpl.KLDivergenceRegularizer(make_mvn_prior(64),use_exact_kl=True, weight=1.0) , \n",
    "               bias_regularizer= tfpl.KLDivergenceRegularizer(make_mvn_prior(64),use_exact_kl=True, weight=1.0) , \n",
    "               activation='relu')(latent_inputs)\n",
    "\n",
    "x = tfkl. Dense(128, \n",
    "        kernel_initializer= tf.random_normal_initializer(mean=0, stddev=0.01),   \n",
    "        bias_initializer=tf.zeros_initializer() ,\n",
    "                kernel_regularizer= tfpl.KLDivergenceRegularizer(make_mvn_prior(128),use_exact_kl=True, weight=1.0) , \n",
    "                bias_regularizer= tfpl.KLDivergenceRegularizer(make_mvn_prior(128),use_exact_kl=True, weight=1.0) , \n",
    "                activation='relu')(x)\n",
    "\n",
    "# ### here uncomment for BAE-homo\n",
    "#x=x+0.01\n",
    "\n",
    " # Here for BAE-Hetero-Latent  or BAE-Hetero-Latent-VLB or BAE-Homo or BAE-None\n",
    "x_recons=tfkl.Dense(original_dim,\n",
    "        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.001),  \n",
    "        bias_initializer=tf.zeros_initializer() ,\n",
    "                    kernel_regularizer= tfpl.KLDivergenceRegularizer(make_mvn_prior(original_dim),use_exact_kl=True, weight=1.0) , \n",
    "                    bias_regularizer= tfpl.KLDivergenceRegularizer(make_mvn_prior(original_dim),use_exact_kl=True, weight=1.0) ,  \n",
    "                    activation='sigmoid')(x)\n",
    "\n",
    "# ###  Here uncomment for BAE-Hetero-Last\n",
    "# x = tfkl. Dense(original_dim, \n",
    "#         kernel_initializer= tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "#         bias_initializer=tf.zeros_initializer() ,\n",
    "#                            kernel_regularizer=tfk.regularizers.L2(reg_L2),  \n",
    "#                 bias_regularizer=tfk.regularizers.L2(reg_L2),\n",
    "#                 activation='relu')(x)\n",
    "# x_recons =tfp.layers.DistributionLambda(\n",
    "#     lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "#                    loc=t, scale_diag=1e-5 + tf.nn.softplus(np.log(np.expm1(1.)) + t) ))(x) \n",
    " \n",
    "    \n",
    "decoder=tfk.Model(latent_inputs, x_recons, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e225e936-43d5-4ac9-b257-c57e39c6ceba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "loss_prior = tfd.Independent(tfd.Normal(loc=tf.zeros(77), scale=1), reinterpreted_batch_ndims=1)\n",
    "\n",
    "class BAE_AD(tfk.Model):\n",
    "    \n",
    "    def __init__(self, orig_dim, kl_weight=1, name=\"bae\", **kwargs):\n",
    "        super(BAE_AD, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.dim_orig=orig_dim\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        #self.kl_weight = kl_weight\n",
    "        self.total_loss_tracker = tfk.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tfk.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        #self.kl_loss_tracker = tfk.metrics.Mean(name=\"kl_loss\")\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            #self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def call(self, x_input):   # auto influence cross entropy value\n",
    "        z_sample=self.encoder(x_input)        \n",
    "        \n",
    "        x_recons= decoder(z_sample   )#.sample() )\n",
    "        \n",
    "        return x_recons\n",
    "    \n",
    "    \n",
    "    def train_step(self,x_true):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_sample=self.encoder(x_true)\n",
    "            x_recons=self. decoder(z_sample  )#.sample())            \n",
    "            \n",
    "            #reconstruction_loss=tf.reduce_mean( tfk.losses.binary_crossentropy(x_true, x_recons))   # crossentropy\n",
    "            reconstruction_loss =tfk.losses.MeanAbsoluteError()(x_true,x_recons)\n",
    "         \n",
    "            total_loss =reconstruction_loss #+loss_para #+kl_loss\n",
    "            \n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)  \n",
    "        #gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        #self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "           # \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ce82b3-2e1f-4a89-9e0f-b50870676668",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MC_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6907dff1-3c9e-42b0-a426-1a02e5161ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate=0.2\n",
    "reg_L2= 1e-7    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb0651-1b02-4f13-83f5-864b0b659913",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d323c1a-b33b-4c1e-a07a-a2365e20873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import RandomNormal, Constant\n",
    "#2layers  128,64,32\n",
    "prior =tfd.Normal(loc=tf.zeros(32), scale=1)   \n",
    "\n",
    "ini_inputs=tfk. Input(shape=(original_dim,))\n",
    "x = tfkl.Dense( 128,\n",
    "        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "       bias_initializer=  tf.zeros_initializer() ,     \n",
    "                           kernel_regularizer= tfk.regularizers.L2(reg_L2),      \n",
    "              bias_regularizer=tfk.regularizers.L2(reg_L2),\n",
    "               activation='relu')(ini_inputs)  \n",
    "x=tfkl.Dropout(dropout_rate)(x,training=True)\n",
    "\n",
    "x=   tfkl.Dense(  64,\n",
    "        kernel_initializer=  tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "        bias_initializer=  tf.zeros_initializer() ,   \n",
    "                           kernel_regularizer=tfk.regularizers.L2(reg_L2),                                 \n",
    "                bias_regularizer=tfk.regularizers.L2(reg_L2),        \n",
    "                activation='relu')(x)          \n",
    "x= tfkl.Dropout(dropout_rate)(x,training=True)\n",
    "x= tfkl.BatchNormalization(momentum=0.95, epsilon=0.005,\n",
    "                                beta_initializer=RandomNormal(mean=0.0, stddev=0.05), \n",
    "                                    gamma_initializer=Constant(value=0.9)\n",
    "                                )(x)\n",
    "\n",
    "x=tfkl.Dense(32,                \n",
    "        kernel_initializer=  tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "        bias_initializer=  tf.zeros_initializer() , \n",
    "                           kernel_regularizer= tfk.regularizers.L2(reg_L2),  \n",
    "               bias_regularizer=tfk.regularizers.L2(reg_L2),\n",
    "                     activation='relu') (x)\n",
    "\n",
    "#  Here uncomment for BAE-None, BAE-Homo, BAE-Heter-Last\n",
    "z_outputs= x \n",
    "    \n",
    "### Here uncomment for BAE-Hetero-Latent                           \n",
    "# z_outputs= tfp.layers.DistributionLambda(\n",
    "#     lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "#                    loc=t, scale_diag=1e-5 + tf.nn.softplus(np.log(np.expm1(1.)) + t) )) (x)\n",
    "                   \n",
    "                            \n",
    "# ### Here uncomment for BAE-Hetero-Latent-VLB\n",
    "# z_outputs=  tfp.layers.DistributionLambda(\n",
    "#     lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "#                    loc=t, scale_diag=1e-5 + tf.nn.softplus(np.log(np.expm1(1.)) + t) ),\n",
    "#       activity_regularizer=tfpl.KLDivergenceRegularizer(make_mvn_prior(32),use_exact_kl=False, weight=1.0)) (x)\n",
    "  \n",
    "encoder=tfk.Model(ini_inputs, z_outputs, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a58d86c-de20-4141-9f1e-5cfe974980fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 32)]              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 47)                6063      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,495\n",
      "Trainable params: 16,495\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder model \n",
    "latent_inputs =tfk. Input(shape=(32,), name='z_sampling')\n",
    "x = tfkl.Dense(64,  \n",
    "        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "        bias_initializer= tf.zeros_initializer() ,   \n",
    "                           kernel_regularizer= tfk.regularizers.L2(reg_L2),     \n",
    "               bias_regularizer=tfk.regularizers.L2(reg_L2),\n",
    "               activation='relu')(latent_inputs)\n",
    "x=tfkl.Dropout(dropout_rate)(x,training=True)\n",
    "\n",
    "x = tfkl. Dense(128, \n",
    "        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "        bias_initializer= tf.zeros_initializer() , \n",
    "                           kernel_regularizer=tfk.regularizers.L2(reg_L2),  \n",
    "                bias_regularizer=tfk.regularizers.L2(reg_L2),\n",
    "                activation='relu')(x)\n",
    "\n",
    "x=tfkl.Dropout(dropout_rate)(x   ,training=True)\n",
    "\n",
    "# ### here uncomment for BAE-homo\n",
    "#x=x+0.01\n",
    "\n",
    "# Here uncomment for BAE-Hetero-Latent or BAE-Hetero-Latent-VLB or BAE-Homo or BAE-None\n",
    "x_recons=tfkl.Dense(original_dim,\n",
    "        kernel_initializer= tfk.initializers.he_normal(),   # tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "        bias_initializer= tf.zeros_initializer() , #tfk.initializers.he_normal(),  #\n",
    "                           kernel_regularizer=tfk.regularizers.L2(reg_L2),   \n",
    "                    bias_regularizer=tfk.regularizers.L2(reg_L2),\n",
    "                    activation='sigmoid')(x)\n",
    "\n",
    "# ###  Here uncomment for BAE-Hetero-Last\n",
    "# x = tfkl. Dense(original_dim, \n",
    "#         kernel_initializer= tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "#         bias_initializer=tf.zeros_initializer() ,\n",
    "#                            kernel_regularizer=tfk.regularizers.L2(reg_L2),  \n",
    "#                 bias_regularizer=tfk.regularizers.L2(reg_L2),\n",
    "#                 activation='relu')(x)\n",
    "# x_recons =tfp.layers.DistributionLambda(\n",
    "#     lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "#                    loc=t, scale_diag=1e-5 + tf.nn.softplus(np.log(np.expm1(1.)) + t) ))(x) \n",
    " \n",
    "\n",
    "decoder=tfk.Model(latent_inputs, x_recons, name='decoder')  \n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db01d5-5200-4e71-aea8-9bd5dac4c806",
   "metadata": {},
   "source": [
    "## model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3620b91-5ef7-4df0-9f25-09179b7ebff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class BAE_AD(tfk.Model):\n",
    "    \n",
    "    def __init__(self, orig_dim, kl_weight=1, name=\"bae\", **kwargs):\n",
    "        super(BAE_AD, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.dim_orig=orig_dim\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        #self.kl_weight = kl_weight\n",
    "        self.total_loss_tracker = tfk.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tfk.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        #self.kl_loss_tracker = tfk.metrics.Mean(name=\"kl_loss\")\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            #self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def call(self, x_input):   # auto influence cross entropy value        \n",
    "        z_sample=self.encoder(x_input)        \n",
    "        x_recons=self.decoder(z_sample  )#.sample() )\n",
    "        return     x_recons    \n",
    "    \n",
    "    \n",
    "    def train_step(self,x_true):\n",
    "        with tf.GradientTape() as tape:                        \n",
    "            z_sample=self.encoder(x_true)\n",
    "            x_recons=self.decoder(z_sample   )#.sample()   )\n",
    "            \n",
    "            #reconstruction_loss=tf.reduce_mean( tfk.losses.binary_crossentropy(x_true, x_recons)  ) # crossentropy\n",
    "            reconstruction_loss =tfk.losses.MeanAbsoluteError()(x_true,x_recons)\n",
    "            \n",
    "            total_loss =reconstruction_loss #+loss_para #+kl_loss\n",
    "            \n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)  \n",
    "        #gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        #self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "           # \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f72aa6e-1a5e-469f-86f5-c15de666543f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16bb2e59-02fc-4ea5-bfd4-592e48b1455e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M=1   # the M=1 when minibatch big enough,>100\n",
    "savename=\"unsw-44-mcd-vae-seed0-mae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42755e0d-1192-4926-afad-c0e8af0f82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "t = MinMaxScaler()\n",
    "t.fit(normal_train_x)\n",
    "normal_train_x = t.transform(normal_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fe575-db56-422d-90c2-04b96464a993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile and train tfk.Model\n",
    "BDL_model = BAE_AD(orig_dim=original_dim)\n",
    "BDL_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001,epsilon=1e-08,amsgrad=True))#decay=1.0/200,  # not good of Adam and decay\n",
    "\n",
    "batchsize=512\n",
    "epochsize=100\n",
    "start=time.time()\n",
    "train_history= BDL_model.fit(normal_train_x, batch_size=batchsize,epochs=epochsize,shuffle=True)\n",
    "end=time.time()\n",
    "print(f\"\\nTime to training: {round(end-start,5)} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06e72f59-3e79-41d0-a855-3894a671b664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pyplot.plot(train_history.history['loss'])\n",
    "pyplot.title('model loss')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.legend(['train', 'test'], loc='upper right');\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c0820b-647c-4c76-b121-d4faff0a7d2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# save or load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68697e41-e501-4dd7-8e2d-893661a335e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the weights\n",
    "BDL_model.save_weights('./unsw_checkpoints/'+savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "018ddc83-cb7d-4718-ad74-ea9b70713f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f0a201f2ee0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BDL_model = BAE_AD(orig_dim=original_dim)\n",
    "oldsavename=\"unsw-4-mae\"# None  # the old file name \n",
    "## Restore the weights\n",
    "BDL_model.load_weights('./unsw_checkpoints/'+oldsavename) \n",
    "#savename='unsw-11-drop-input-mae' # the future name\n",
    "\n",
    "## Evaluate the model\n",
    "#loss, acc = BDL_model.evaluate(normal_train_x)\n",
    "#print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74d528-3512-4d97-9b7f-fd38148abf8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# function set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "370a1c2e-d464-469f-b9e4-42867f640c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attack_types=['Normal','Shellcode','Exploits','Fuzzers','Reconnaissance','Generic','Worms','Analysis','DoS','Backdoors','AllAttacks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f129df2-8ea0-45bd-b3a4-4a438944bcbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def metric3UQ(fileforname, skiprow=None):\n",
    "    pd_reader=pd.read_csv('../../pplots/BDL'+fileforname+'+allmetrics.csv'   ,skiprows=skiprow)   \n",
    "    #print(pd_reader.head())\n",
    "    save_path=\"pplots/BDL\"+fileforname+'+allmetricsSum'+\".csv\"\n",
    "    \n",
    "    modelname='BDL'\n",
    "    model_dict_total={}\n",
    "    model_dict_alea={}\n",
    "    model_dict_epis={}\n",
    "    model_dict_total[\"metricname\"]=pd_reader.iloc[17,1:19]\n",
    "    model_dict_alea[\"metricname\"]=pd_reader.iloc[17,1:19]\n",
    "    model_dict_epis[\"metricname\"]=pd_reader.iloc[17,1:19]\n",
    "    for each in range(1,len( attack_types)):\n",
    "        fig, (ax1, ax2, ax3, ax4) = pyplot.subplots(1,4,figsize=(14,2.5),sharey=True) \n",
    "        fig.suptitle(attack_types[each] ,x=-0.05,y=0.5) # {typename} {modelname}')\n",
    "\n",
    "        for i in range(1,len(pd_reader),18*3):\n",
    "            if '+' in pd_reader.iloc[i,0] :\n",
    "\n",
    "                if pd_reader.iloc[i,0].split('+')[1].split('.')[0]==attack_types[each]:\n",
    "                    legendd=pd_reader.iloc[i,0].split('+')[0]\n",
    "\n",
    "                    if legendd in model_dict_total.keys():\n",
    "                        model_dict_total[legendd]=model_dict_total[legendd]+pd_reader.iloc[i+17,1:19].astype(np.float16)\n",
    "                        model_dict_alea[legendd]=model_dict_alea[legendd]+pd_reader.iloc[i+17+18,1:19].astype(np.float16)\n",
    "                        model_dict_epis[legendd]=model_dict_epis[legendd]+pd_reader.iloc[i+17+18+18,1:19].astype(np.float16)\n",
    "\n",
    "                    else: \n",
    "                        model_dict_total[legendd]=pd_reader.iloc[i+17,1:19].astype(np.float16)    \n",
    "                        model_dict_alea[legendd]=pd_reader.iloc[i+17+18,1:19].astype(np.float16) \n",
    "                        model_dict_epis[legendd]=pd_reader.iloc[i+17+18*2,1:19].astype(np.float16)  \n",
    "\n",
    "                    #fig_title=pd_reader.iloc[i,0]\n",
    "                    try:\n",
    "                        xx=pd_reader.iloc[i+2,1:].astype(int)\n",
    "                    except:\n",
    "                        print(i)\n",
    "\n",
    "                    # total uncertainty\n",
    "                    y_uncer=pd_reader.iloc[i+15,1:].astype(np.float16)\n",
    "                    y_uncer2=pd_reader.iloc[i+15+18,1:].astype(np.float16)  # alea\n",
    "                    y_uncer3=pd_reader.iloc[i+15+18*2,1:].astype(np.float16)   #epis\n",
    "                    #plot_title=pd_reader.iloc[i+2,0]            \n",
    "                    #y_values=pd_reader.iloc[i+3,1:].astype(np.float16)  # auc\n",
    "                    y_values=pd_reader.iloc[i+9,1:].astype(np.float16)   # accu_total\n",
    "                    y_values2=pd_reader.iloc[i+9+18,1:].astype(np.float16)   # accu_alea\n",
    "                    y_values3=pd_reader.iloc[i+9+18*2,1:].astype(np.float16)   # accu_epis\n",
    "\n",
    "                    # ACCU_total\n",
    "                    ax1.plot(xx,y_values,label=legendd)                \n",
    "                    ax1.fill_between(xx,y_values-np.absolute(y_uncer),y_values+np.absolute(y_uncer),alpha=0.3,facecolor='grey')\n",
    "                    ax1.set(ylabel='Percent')              \n",
    "                    # ACCU_alea\n",
    "                    ax2.plot(xx, y_values2) # , 'tab:green')        \n",
    "                    ax2.fill_between(xx,y_values2-np.absolute(y_uncer2),y_values2+np.absolute(y_uncer2),alpha=0.3,facecolor='grey')        \n",
    "                    # ACCU_epis\n",
    "                    ax3.plot(xx, y_values3 ) #, 'tab:orange') \n",
    "                    ax3.fill_between(xx,y_values3-np.absolute(y_uncer3),y_values3+np.absolute(y_uncer3),alpha=0.3,facecolor='grey')         \n",
    "\n",
    "                    # AUC\n",
    "                    ax4.plot(xx, pd_reader.iloc[i+3,1:].astype(np.float16) ) # , 'tab:red') \n",
    "\n",
    "                    #ax4.plot(xx,pd_reader.iloc[i+5,1:].astype(np.float16),label=legendd)  #GSS for test\n",
    "\n",
    "                    if each==1:\n",
    "                        ax1.set_title('Total')\n",
    "                        ax2.set_title('Aleatoric') \n",
    "                        ax3.set_title('Epistemic')\n",
    "                        ax4.set_title('AUC') \n",
    "                    if each==len(attack_types)-1:\n",
    "                        ax1.set(xlabel='Rejection')\n",
    "                        ax2.set(xlabel='Rejection')\n",
    "                        ax3.set(xlabel='Rejection')\n",
    "                        ax4.set(xlabel='Rejection')\n",
    "                   \n",
    "                    #i=i+18*3  \n",
    "\n",
    "\n",
    "        handles, labels = ax1.get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, loc=\"lower right\" , bbox_to_anchor=(1.03,0.4))\n",
    "\n",
    "        for ax in fig.get_axes():\n",
    "            ax.label_outer()\n",
    "            ax.set_xlim(0,60)\n",
    "            ax.set_ylim(0,1)\n",
    "        \n",
    "        pyplot.show()\n",
    "\n",
    "    for aproblegend in model_dict_total.keys():\n",
    "        if aproblegend != 'metricname':\n",
    "            model_dict_total[aproblegend]=model_dict_total[aproblegend]/(len(attack_types)-1)\n",
    "            model_dict_alea[aproblegend]=model_dict_alea[aproblegend]/(len(attack_types)-1)\n",
    "            model_dict_epis[aproblegend]=model_dict_epis[aproblegend]/(len(attack_types)-1)\n",
    "\n",
    "    save_data=pd.DataFrame(model_dict_total)\n",
    "    save_data.to_csv(save_path, mode='a', header=True, index_label=modelname,index=True)\n",
    "    save_data=pd.DataFrame(model_dict_alea)\n",
    "    save_data.to_csv(save_path, mode='a', header=True, index_label=modelname,index=True)\n",
    "    save_data=pd.DataFrame(model_dict_epis)\n",
    "    save_data.to_csv(save_path, mode='a', header=True, index_label=modelname,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28f36875-2c86-4f6a-b2f8-14aaef758c2c",
   "metadata": {
    "id": "KSnlaC33eCtV"
   },
   "outputs": [],
   "source": [
    "def get_error_term(v1,v2, _rmse=True):\n",
    "    if _rmse:\n",
    "        return np.sqrt(np.mean((v1-v2)**2,axis=1))\n",
    "    else:        #return MAE\n",
    "        return np.mean(abs(v1-v2),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f98d163-af44-4a29-b4a3-b11a1ffadf55",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1.testing+more dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427dba1-375d-4d68-a97e-9abb15faea4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "alluncertainty_1_dict={}     # record all UQ under multiple distributions on anoamly score. using rmse\n",
    "all_AS_AP_1_dict={}    # record all anoamly scores and anoamly probabilities in T times repeats. using rmse\n",
    "all_AS_AP_2_dict={}  # using MAE to calculate anoamly score\n",
    "\n",
    "all_test_y_2_dict={}\n",
    "newname=savename  +'-mae'\n",
    "T=  M      # the times of prediction, i.e. the number of parameters samples.\n",
    "distribution_types=['G1','G2','Gamma','Uniform','T','ECDF', 'Triang']       #  'GC',,'ECOD'\n",
    "'''\n",
    "G1: standard Gaussian, i.e. mean=0,std=1\n",
    "G2: customized Gaussian using CDF()\n",
    "'''\n",
    "#start to timeing\n",
    "start=time.time()\n",
    "for root,dirs,files in os.walk(dicnpy):\n",
    "    for each in files:\n",
    "        if (dicnpy+each).endswith('.npy'):        \n",
    "            #temp=pd.concat([single_group,normal_train],axis=0)\n",
    "            if each!='Normal.npy' and each.split('_')[0]!='Normal':\n",
    "            #if each=='Shellcode.npy':\n",
    "                print(each)\n",
    "                cont=np.load(dicnpy+each,allow_pickle=True)\n",
    "                testdata=np.concatenate([cont,normal_test],axis=0)  \n",
    "                test_x,test_y=column_split_bina(testdata)\n",
    "                if type(test_x)==False:\n",
    "                    print('test_x is False!')\n",
    "                \n",
    "                all_test_y_2_dict[each]=test_y\n",
    "                # scale data\n",
    "                t = MinMaxScaler()\n",
    "                t.fit(test_x)\n",
    "                test_x2 = t.transform(test_x)\n",
    "                # for rmse\n",
    "                mean_1_G1, mean_1_G2, mean_1_Gamma, mean_1_Uniform,mean_1_Triang, mean_1_T,mean_1_ECDF=0,0,0,0,0,0,0\n",
    "                mean_1_AS=0     # the averaging of anomaly scores\n",
    "              \n",
    "                for i in range(0,T):\n",
    "                    test_recon=BDL_model.predict(test_x2)\n",
    "                       \n",
    "                \n",
    "                    AS_1_vector = get_error_term(test_recon, test_x2, _rmse=False) \n",
    "                    all_AS_AP_1_dict[each+'_AS_'+str(i)]=AS_1_vector\n",
    "                    mean_1_AS=mean_1_AS+AS_1_vector\n",
    "                    \n",
    "                    AS_2_vector = get_error_term(test_recon, test_x2, _rmse=True) \n",
    "                    all_AS_AP_2_dict[each+'_AS_'+str(i)]=AS_2_vector\n",
    "                \n",
    "                    # test 2.1------------------------standard Gaussian------------------------------------\n",
    "                    \n",
    "                    AS_1_vector_prob=copy.deepcopy(AS_1_vector)  \n",
    "                    #AS_1_vector_prob=regular_log(AS_1_vector)   ######################\n",
    "                    #AS_1_vector_prob=regular_linear(AS_1_vector)                   \n",
    "                    # 1.the anomaly scores following Gaussian distribution                  \n",
    "                    AS_1_vector_prob=normalization_gaussian_1( AS_1_vector_prob)     \n",
    "                    all_AS_AP_1_dict[each+'_AP_'+str(i)+'_G1']=AS_1_vector_prob\n",
    "                    mean_1_G1=mean_1_G1+AS_1_vector_prob\n",
    "                    \n",
    "                   \n",
    "                     #-------------------Customized Gaussian using CDF-----------------------------------------\n",
    "                    \n",
    "                    AS_1_vector_prob=copy.deepcopy(AS_1_vector)  \n",
    "                    #AS_1_vector_prob=regular_log(AS_1_vector)   ######################\n",
    "                    #AS_1_vector_prob=regular_linear(AS_1_vector)                   \n",
    "                    # 2.the anomaly scores following Gaussian distribution\n",
    "                    AS_1_vector_prob=normalization_gaussian_2(AS_1_vector_prob)\n",
    "                    all_AS_AP_1_dict[each+'_AP_'+str(i)+'_G2']=AS_1_vector_prob \n",
    "                    mean_1_G2=mean_1_G2+AS_1_vector_prob\n",
    "                                        \n",
    "                    # 4.the anomaly scores following Gamma distribution -----------------------------------------------\n",
    "                    #AS_1_vector_prob=regular_log(AS_1_vector)\n",
    "                    #AS_1_vector_prob=regular_linear(AS_1_vector)\n",
    "                    AS_1_vector_prob=copy.deepcopy(AS_1_vector )                       \n",
    "                    AS_1_vector_prob=normalization_gamma(AS_1_vector_prob)\n",
    "                    all_AS_AP_1_dict[each+'_AP_'+str(i)+'_Gamma']=AS_1_vector_prob\n",
    "                    mean_1_Gamma=mean_1_Gamma+AS_1_vector_prob\n",
    "\n",
    "                                                                    \n",
    "                     # 6.the anomaly scores following uniform distribution -----------------------------------  \n",
    "                    #AS_1_vector_prob=regular_log(AS_1_vector)\n",
    "                    #AS_1_vector_prob=regular_linear(AS_1_vector)\n",
    "                    AS_1_vector_prob=copy.deepcopy(AS_1_vector)                          \n",
    "                    AS_1_vector_prob=normalization_uniform(AS_1_vector_prob)\n",
    "                    all_AS_AP_1_dict[each+'_AP_'+str(i)+'_Uniform']=AS_1_vector_prob\n",
    "                    mean_1_Uniform=mean_1_Uniform+AS_1_vector_prob\n",
    "                    \n",
    "                     # 7.the anomaly scores following Triang distribution   ----------------------------------   \n",
    "                        \n",
    "                    AS_1_vector_prob=copy.deepcopy(AS_1_vector )  \n",
    "                    #AS_1_vector_prob=regular_log(AS_1_vector)\n",
    "                    #AS_1_vector_prob=regular_linear(AS_1_vector)                       \n",
    "                    AS_1_vector_prob=normalization_triang(AS_1_vector_prob)\n",
    "                    all_AS_AP_1_dict[each+'_AP_'+str(i)+'_Triang']=AS_1_vector_prob\n",
    "                    mean_1_Triang=mean_1_Triang+AS_1_vector_prob\n",
    "                    \n",
    "                     # 8.the anomaly scores following students  t distribution   ----------------------------------  \n",
    "                    #AS_1_vector_prob=regular_log(AS_1_vector)\n",
    "                    #AS_1_vector_prob=regular_linear(AS_1_vector)\n",
    "                    AS_1_vector_prob=copy.deepcopy(AS_1_vector)                     \n",
    "                    AS_1_vector_prob=normalization_t(AS_1_vector_prob)\n",
    "                    all_AS_AP_1_dict[each+'_AP_'+str(i)+'_T']=AS_1_vector_prob\n",
    "                    mean_1_T=mean_1_T+AS_1_vector_prob\n",
    "                    \n",
    "                   \n",
    "                    '''\n",
    "                    # test 9 -----------------------pure CDF-------------------------------------\n",
    "                    # the probability without normalization is not right in theory. not suit to judge as prabability, but with a central range.\n",
    "                    #3. alternative: pure cdf()   \n",
    "                    #AS_1_vector_prob=stats.norm.cdf(AS_1_vector)  \n",
    "                    '''\n",
    "                    #-------------------------------ECDF-----------------------------------------\n",
    "                    # 3.1 ecdf       \n",
    "                    #AS_1_vector_prob=regular_log(AS_1_vector)\n",
    "                    AS_1_vector_prob=regular_linear(AS_1_vector)\n",
    "                    AS_1_vector_prob=normalization_ECDF(AS_1_vector_prob)\n",
    "                    #AS_1_vector_prob=cdf_ECDF( AS_1_vector_prob)                     \n",
    "                    all_AS_AP_1_dict[each+'_AP_'+str(i)+'_ECDF']=AS_1_vector_prob\n",
    "                    mean_1_ECDF=mean_1_ECDF+AS_1_vector_prob\n",
    "                    \n",
    "                mean_AP={}\n",
    "                mean_AP['AS']=mean_1_AS/T\n",
    "                mean_AP['G1']=mean_1_G1/T\n",
    "                mean_AP['G2']=mean_1_G2/T\n",
    "                mean_AP['Gamma']=mean_1_Gamma/T\n",
    "                mean_AP['Uniform']=mean_1_Uniform/T\n",
    "                mean_AP['Triang']=mean_1_Triang/T\n",
    "                mean_AP['T']=mean_1_T/T\n",
    "                mean_AP['ECDF']=mean_1_ECDF/T\n",
    "                mean_1_G1, mean_1_G2, mean_1_Gamma, mean_1_Uniform,mean_1_Triang, mean_1_T,mean_1_ECDF=0,0,0,0,0,0,0\n",
    "                mean_1_AS=0     # the averaging of anomaly scores\n",
    "               \n",
    "                mean_1_alea={}\n",
    "                mean_1_epis={}\n",
    "                for dist in distribution_types:\n",
    "                    mean_1_alea[dist]=0\n",
    "                    mean_1_epis[dist]=0\n",
    "                    \n",
    "                mean_AS_1_alea=0\n",
    "                mean_AS_1_epis=0\n",
    "                    \n",
    "                for i in range(0,T):  \n",
    "                    mean_AS_1_alea+= all_AS_AP_1_dict[each+'_AS_'+str(i)]*(1-all_AS_AP_1_dict[each+'_AS_'+str(i)])  # for Bonulli distribution  #(1-np.absolute(all_AS_AP_dict[each+'_AP_'+i+'_G1']))   \n",
    "                    mean_AS_1_epis +=(all_AS_AP_1_dict[each+'_AS_'+str(i)] - mean_AP['AS'])  **2  \n",
    "                    print(all_AS_AP_1_dict[each+'_AS_'+str(i)])\n",
    "                    print(mean_AP['AS'])\n",
    "                    \n",
    "                    for dist in distribution_types:\n",
    "                        if all_AS_AP_1_dict[each+'_AP_'+str(i)+'_'+dist].ndim>1:   # normally for image\n",
    "                            # eq.7,8 in https://openreview.net/pdf?id=Sk_P2Q9sG\n",
    "                            mean_1_alea[dist] += np.mean(all_AS_AP_1_dict[each+'_AP_'+str(i)+'_'+dist]* (1-all_AS_AP_1_dict[each+'_AP_'+str(i)+'_'+dist]),axis=1)   #(1-np.absolute(all_AS_AP_dict[each+'_AP_'+i+'_G1'])), axis=1)\n",
    "                            mean_1_epis[dist] += (np.mean(all_AS_AP_1_dict[each+'_AP_'+str(i)+'_'+dist],axis=1)- np.mean(mean_AP[dist], axis=1))    **2 \n",
    "                            \n",
    "                        else:\n",
    "                            mean_1_alea[dist] += all_AS_AP_1_dict[each+'_AP_'+str(i)+'_'+dist]*(1-all_AS_AP_1_dict[each+'_AP_'+str(i)+'_'+dist])    #(1-np.absolute(all_AS_AP_dict[each+'_AP_'+i+'_G1']))   \n",
    "                            mean_1_epis[dist] +=(all_AS_AP_1_dict[each+'_AP_'+str(i)+'_'+dist] - mean_AP[dist])    **2    \n",
    "                                                  \n",
    "                mean_AS_1_alea=mean_AS_1_alea/T\n",
    "                mean_AS_1_epis=mean_AS_1_epis/T\n",
    "                alluncertainty_1_dict[each+'_AS_alea']=mean_AS_1_alea\n",
    "                alluncertainty_1_dict[each+'_AS_epis']=mean_AS_1_epis\n",
    "                result_dic=plot_metrics(modelname='BDL'+newname,typename='AS+'+each, ascores=mean_AP['AS'], \n",
    "                                            unc_total=(mean_AS_1_alea+mean_AS_1_epis), y_true=test_y,plot=False) \n",
    "                result_dic=plot_metrics(modelname='BDL'+newname,typename='AS+'+each, ascores=mean_AP['AS'], \n",
    "                                            unc_total=mean_AS_1_alea, y_true=test_y,plot=False) \n",
    "                result_dic=plot_metrics(modelname='BDL'+newname,typename='AS+'+each, ascores=mean_AP['AS'], \n",
    "                                            unc_total=mean_AS_1_epis, y_true=test_y,plot=False) \n",
    "                \n",
    "                              \n",
    "                for dist in distribution_types:\n",
    "                    mean_1_alea[dist]=mean_1_alea[dist]/T\n",
    "                    mean_1_epis[dist]=mean_1_epis[dist]/T                    \n",
    "                    alluncertainty_1_dict[each+'_'+dist+'_alea']=mean_1_alea[dist] \n",
    "                    alluncertainty_1_dict[each+'_'+dist+'_epis']=mean_1_epis[dist]       \n",
    "                    uncer_1_total=mean_1_alea[dist] +mean_1_epis[dist]\n",
    "                    result_dic=plot_metrics(modelname='BDL'+newname,typename=dist+'+'+each, ascores=mean_AP[dist], \n",
    "                                            unc_total=uncer_1_total, y_true=test_y,plot=False)  #uncer_total\n",
    "                    result_dic=plot_metrics(modelname='BDL'+newname,typename=dist+'+'+each, ascores=mean_AP[dist], \n",
    "                                            unc_total=mean_1_alea[dist], y_true=test_y,plot=False) \n",
    "                    result_dic=plot_metrics(modelname='BDL'+newname,typename=dist+'+'+each, ascores=mean_AP[dist], \n",
    "                                unc_total=mean_1_epis[dist], y_true=test_y,plot=False) \n",
    "                    \n",
    "                 #break\n",
    "end=time.time()                 \n",
    "runTime=pd.DataFrame({'ModelName':['BDL'+savename],'Phase':['test'],'M':[T],'DistNum':[len(distribution_types)],'time':[round(end-start,5)],'batch':[0],'epoch':[0]})\n",
    "save_path = 'pplots/allRunningTimes'+'.csv'\n",
    "csv_exists = os.path.exists(save_path)\n",
    "csv_mode = 'a' if csv_exists else 'w'\n",
    "header_mode = False if csv_exists else True\n",
    "runTime.to_csv(save_path, mode=csv_mode, header=header_mode , index_label=end,index=True)\n",
    "\n",
    "del result_dic,   mean_1_G1, mean_1_G2, mean_1_Gamma, mean_1_Uniform,mean_1_Triang, \n",
    "del mean_1_T,mean_1_ECDF, mean_1_AS    \n",
    "del AS_1_vector, AS_2_vector, AS_1_vector_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "208bb85f-7f56-4923-90fa-832a70ff8340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "uncer_save_path = \"pplots/\"+'BDL'+newname+'+alluncer'+\".csv\"  # to be saved path\n",
    "ASAP_save_path = \"pplots/\"+'BDL'+newname+'+ASAP'+\".csv\"  # to be saved path\n",
    "# save all uncertainty to csv file\n",
    "save_uncer = pd.DataFrame.from_dict(alluncertainty_1_dict, orient='index')\n",
    "save_uncer = save_uncer.transpose()\n",
    "save_uncer.to_csv(uncer_save_path, mode='a', header=True, index_label=newname,index=True)\n",
    "# save all anomaly scores and anomaly probabilities\n",
    "save_ASAP = pd.DataFrame.from_dict(all_AS_AP_1_dict, orient='index')\n",
    "save_ASAP = save_ASAP.transpose()\n",
    "save_ASAP.to_csv(ASAP_save_path, mode='a', header=True, index_label=newname,index=True)\n",
    "\n",
    "del save_uncer,save_ASAP,alluncertainty_1_dict,all_AS_AP_1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3f75a-eb01-462f-99dd-f0433cbf614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newname)\n",
    "metric3UQ(newname)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3814c-aa76-4192-9569-d7042da1d39a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
